---
title: "Causal Inference Fundamentals"
output:
  html_document:
    theme: cerulean
    highlight: kate
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{=html}
<style>
body {
  font-size: 17px;
  line-height: 1.6;
}
</style>
```

```{r setup, include=FALSE}
library(kernlab)
library(showtext)
library(kableExtra)
library(dplyr)
library(tidyverse)
library(scales)
```

## Causal Inference Fundamentals

In this module, we explore what it means to make a causal claim in the social sciences—and how we can go about evaluating whether one thing truly causes another. We’ll begin by distinguishing causality from correlation and move on to different types of causal definitions, the core assumptions required for estimating causal effects, and finally, best practices for building your own causal theories.

## Part 1: What is Causality?

Causality is about understanding how and why one thing influences another. Rather than simply observing patterns in the data, causal inference asks: What would have happened if things had been different?

**Correlation is Not Causation**

It’s important not to confuse correlation with causation. Just because two things tend to happen at the same time doesn’t mean that one causes the other. To really understand causality, we have to dig deeper and ask: Is one thing actually producing a change in the other, or are they just happening side by side because of something else?

```{r, echo=FALSE, fig.align='center', out.width='80%'}
knitr::include_graphics("/Users/jieun/Documents/MAIS_POLI706/figures/icecream.jpg")
```

Let’s take a simple example. Imagine we notice that when ice cream sales go up, the number of sunburn cases also increases. At first glance, this might seem like eating more ice cream somehow causes people to get sunburned. But of course, that doesn’t make sense!

What’s really going on is that both of these things—buying ice cream and getting sunburned—are influenced by a third factor: hot, sunny weather. When the sun is out and temperatures are high, people flock outdoors. They’re more likely to enjoy cold treats like ice cream, and they’re also more likely to spend time in the sun, which increases their chances of getting sunburned. So while there’s a correlation between ice cream sales and sunburns, one doesn’t cause the other.

This is why, when we study causal relationships—especially in political science, economics, or health—we need to go beyond surface-level patterns. We must ask whether there's a plausible mechanism connecting the variables, and whether other explanations might be behind the patterns we see.

In short: correlation can be a useful clue, but it’s not the whole story. To make strong causal claims, we need to do a bit more detective work.

Causal inference helps us rule out spurious relationships and focus on the variables that truly affect our outcome of interest.

**Key Terms**

Dependent Variable (Outcome Variable): This is the main variable you're trying to explain. It represents the result or consequence of other factors. For instance, if you're studying civil conflict, your outcome might be whether or not a country experienced a civil war in a given year.

Explanatory Variables: These are the factors you think might explain or influence the dependent variable. Among these:

- The key causal variable (or treatment) is the main factor you hypothesize has a causal effect.

- Control variables are other variables that might influence the outcome and need to be accounted for to isolate the effect of the key causal variable.

Understanding what causality means, and how it's different from mere association, is the first step toward asking better research questions and designing better studies.

### Descriptive vs. Causal Inference

#### Descriptive Inference

**Descriptive inference** is about learning something that we have not directly observed, based on what we have observed. It goes beyond simply summarizing patterns in the data—it uses those patterns to make informed claims about unknown or unobserved facts.

In Module 2, we practiced descriptive inference by studying the relationship between civil war and the capacity of Election Management Bodies (EMBs). While we visualized and compared EMB capacity scores between countries with and without civil war, we inferred that EMB capacity is correlated with civil war onset. From this observation, we could say:

*“Countries with civil wars tend to have lower EMB capacity scores than those without civil wars.”*

We didn’t just stop at describing the scores in our dataset — we used the patterns in that sample to say something meaningful about the broader population of countries. That’s what makes it descriptive inference.

However, even with statistically significant results, our inference remained descriptive—not causal. We learned that there is an association between civil war and EMB capacity, but we haven’t answered the why. Is civil war causing weaker institutions? Are weak institutions making civil war more likely? Or are both driven by a third factor, like poor governance or economic instability?

#### Causal Inference

**Causal inference** takes things a step further and asks deeper questions:

- What would have happened if something had been different?

- Would a country have avoided a financial crisis if it had implemented stricter banking regulations?

- Would voter turnout have increased if election day had been declared a national holiday?

It’s not enough to observe patterns—we want to understand **the cause** behind those patterns.

For example, it’s descriptive to say that countries with strong EMB capacity tend to experience fewer civil wars. But it’s causal to ask: Does strengthening EMB capacity reduce the likelihood of civil war? That’s the heart of causal inference—trying to isolate the effect of one factor on another, while ruling out other possible explanations.

Here, the researcher is not only describing what is happening, but also trying to estimate what would happen if we changed something—what happens to Y when we change X.

In short, descriptive inference tells us what is happening, while causal inference helps us understand why it’s happening—and what might happen if we intervene or make a change. That shift—from observing the world to imagining how it might change—is what makes causal inference such a powerful tool in political science and beyond.

### Counterfactual Conditions

Causal inference is all about asking, “What would have happened if things had been different?” This is what we call a **counterfactual question**.

For example, imagine a country that experienced a civil war and scored low on Election Management Body (EMB) capacity. Now we might wonder: Would this country have had a higher EMB capacity if it hadn’t gone through a civil war? That’s a causal question. But here’s the challenge: we only ever see what actually happened—not what could have happened under a different scenario. We don’t get to observe both versions of the world for the same country.

This gap is known as **the fundamental problem of causal inference.** For every country, we only observe one outcome: either it had a civil war or it didn’t. But to understand causality, we need to compare what did happen to what would have happened. Since we can’t directly observe the counterfactual, we need to estimate it using data from other, similar units—and this is where good research design comes in.

### Realized Causal Effects

A realized causal effect is the difference between two outcomes: one that we observed, and one that we didn’t but wish we could have.

For unit $i$, the realized causal effect is 

$$
y_i^I - y_i^N
$$

Where $y_i^I$ is the actual observation, and $y_i^N$ is the counterfactual situation.

Let’s take our earlier example. Suppose Country A experienced civil war in the year 1990 and scored 1.2 on EMB capacity. What would its score have been if it hadn’t had a civil war? Maybe it would have been 2.0. The realized causal effect would then be:

$$
\text{Causal Effect} = 1.2 \text{(actual)} - 2.0 \text{(counterfactual)} = -0.8
$$

But remember — we don’t actually get to observe that counterfactual 2.0. We have to estimate it, perhaps by looking at countries similar to Country A that didn't experience civil war. This is the fundamental problem of causal inference. No matter how perfect the research design, no matter how much data we collect, no matter how perceptive the observers, and no matter how much experimental control we have, we will never know the causal inference for certain. 

### Random Causal Effects

In social science, it helps to think about the world as made up of both **systematic** and **nonsystematic** parts. To see why this matters, imagine a parallel universe where Country A goes through a civil war in 1990. In that world, the country's EMB ends up with a certain capacity score. Now imagine we could replay that same scenario — same country, same civil war — across many different versions of the universe. Even though the civil war always happens, the EMB capacity score might turn out differently each time. Why? Because of things we can't fully predict or control — like what neighboring countries were doing or if the country’s leader suddenly changed. These kinds of features are what we call nonsystematic features, and they make outcomes vary even when the main treatment stays the same.

These features reflect the randomness not captured by our theoretical model. We can therefore imagine a variable that would express the values of the EMB capacity score across hypothetical replications of this same situation. This variable is called a **"random variable"** since it has nonsystematic features: it is affected by explanatory variables not encompassed in our theoretical analysis or contains fundamentally unexplainable variability. We define the random variable representing the EMB capacity score under civil war as $Y_i^I$ and under peace as $Y_i^N$.

The **random causal effect** would then be : 

$$
Y_i^I - Y_i^N
$$

This variation in outcomes reflects the random causal effect — the difference caused by civil war in a single replication, influenced by unpredictable factors. When we imagine running many versions of Country A with civil war and many versions without civil war, each with slightly different outcomes, what we’re really doing is building up a sense of how the realized causal effect (in one instance) becomes a random causal effect (across many possibilities).

### Mean Causal Effects

The **mean causal effect** takes things a step further. Instead of looking at just one pair of hypothetical outcomes, it asks: *On average, how does civil war affect EMB capacity?*

We again define:

- $Y_i^I$: the EMB capacity score for country $i$ **if** civil war occurs  

- $Y_i^N$: the EMB capacity score for country $i$ **if** civil war does not occur

Then the **mean causal effect** for unit $i$ is the expected value (the average EMB capacity score across these replications) of the random causal effect:

$$
\text{Mean Causal Effect}_i = E(Y_i^I - Y_i^N)
$$



## Part 2: Definitions of Causality

Now that we’ve established what causality means in terms of counterfactual reasoning, we can turn to several important ideas that often come up when people talk about causation. These include concepts like causal mechanisms (how a cause leads to an effect) and multiple causation (how different combinations of factors can produce the same outcome). While these ideas enrich our understanding of causal relationships, they do not alter the core definition of causality as a comparison between what did happen and what would have happened under different circumstances. In the sections that follow, we’ll explore each of these concepts in more detail.

### Causal Mechanisms

A causal mechanism refers to the process through which a cause produces an effect. It represents the sequence of events or interactions that connect the explanatory variable (cause) to the outcome (effect). Identifying mechanisms helps researchers explain how or why a causal relationship exists, providing theoretical depth and empirical clarity.

For instance, suppose a study finds that countries with fewer civil wars tend to have stronger Election Management Bodies (EMBs). A possible causal mechanism might be that the absence of civil war creates a more stable political environment, which allows institutions like EMBs to function more effectively, receive more resources, and gain public trust. In this chain, political stability plays a crucial role in enabling stronger EMB capacity.

While causal mechanisms are valuable for theory development and qualitative analysis (such as through process tracing or case studies), they do not replace the need for a precise definition of causality. Identifying causal mechanisms requires prior causal inference for each step in the chain. Moreover, because an infinite number of steps could lie between a cause and an effect, relying solely on mechanisms without a foundational causal framework can lead to infinite regress.

In short, while mechanisms help illustrate and support a causal story, they are not themselves a definition of causality. The core remains a counterfactual comparison between what happened and what would have happened under a different condition.

### Multiple Causation

Multiple causation refers to situations in which an outcome is influenced by more than one factor. This is especially common in the social sciences, where political, economic, and social outcomes rarely have a single cause. Instead, they emerge from a combination of conditions acting together.

For example, the strength of a country’s Election Management Body (EMB) might depend on several factors: the level of democracy, absence of civil conflict, economic capacity, historical legacies, and international pressure. No single factor may be sufficient on its own, but certain combinations can lead to the same outcome—a phenomenon often called equifinality.

Acknowledging multiple causation does not challenge the counterfactual definition of causality. It simply means we must be more careful in specifying the alternative conditions we compare. Each causal claim still relies on comparing what did happen with what would have happened under a different set of conditions—holding other relevant factors constant.

In short, multiple causation highlights the complexity of real-world causal relationships. It reminds us that we often need to examine how different factors interact and combine, but it does not require changing our core definition of causality. Instead, it reinforces the importance of clearly defining the specific causal effect we aim to estimate.

## Part 3: Assumptions Required for Estimating Causal Effects

To make valid causal claims from our data, we must rely on certain assumptions. These assumptions are often invisible but absolutely essential—they help us overcome the fundamental problem of causal inference, which is that we never observe the same unit both in the treated and untreated condition. Without assumptions, causal inference is impossible.

In this section, we discuss two critical assumptions that help us estimate causal effects: **Unit Homogeneity** and **Conditional Independence**

### Unit Homogeneity

Unit homogeneity assumes that units with the same value of the explanatory variable will have the same expected value of the outcome variable. That is, if two countries both experience civil war (or both do not), and are otherwise comparable, we assume their EMB capacity (or other outcome of interest) should be the same on average.

This assumption allows us to substitute similar units for the counterfactual we can’t observe. For example, suppose we want to estimate the effect of civil war on EMB capacity. We can't rewind history to see what would have happened to Country A if it hadn’t experienced a civil war. But if Country B is similar in all important ways and did not have a civil war, we can treat Country B as a stand-in for Country A’s counterfactual condition.

Of course, no two countries (or individuals, or cases) are truly identical. At best, we assume that, across many replications or in large samples, the average differences between treatment and control groups are meaningful.

Unit homogeneity is foundational to much of social science. Whether we’re comparing case studies or running regressions, we rely on some version of this idea—that observed differences in outcomes are due to differences in explanatory variables, not due to underlying, unobserved heterogeneity between units.

### Conditional Independence 

To estimate causal effects accurately, we must account for **confounders**—variables that influence both the treatment and the outcome. A confounder creates a spurious association by affecting both sides of the causal relationship we care about. If not addressed, confounders can lead us to falsely attribute the effect of the confounder to the treatment variable. 

The assumption of conditional independence helps us tackle this issue. Conditional independence means that, after controlling for all relevant confounding variables, the assignment of the treatment is independent of the potential outcomes. In other words, once we adjust for the right covariates, it should no longer matter whether a unit is treated or not—the treatment should be "as if randomly assigned" given those controls.

For example, imagine we are studying whether civil war reduces EMB capacity. One potential confounder is state capacity: countries with weak state capacity may be more likely to experience civil war and to have low EMB capacity. If we don’t control for state capacity, we might mistakenly conclude that civil war reduces EMB effectiveness, when in fact, both are outcomes of weak capacity. By including state capacity as a control variable, we reduce this risk and move closer to identifying the true causal effect of civil war.

In randomized experiments, this condition is satisfied by design. Random assignment breaks the link between confounders and treatment, ensuring that differences in outcomes can be attributed to the treatment itself. But in observational studies, we don’t get to assign treatments. Instead, we must statistically control for confounding variables that might bias our estimates—such as GDP per capita, regime type, or prior history of violence—when estimating the effect of civil war on EMB capacity.

## Part 4: Rules for Constructing Causal Theories

Now that we understand what causality means and what assumptions are needed for causal inference, how do we go about building strong causal theories? In this section, we outline five key rules that help formulate effective and scientifically valuable causal theories. These rules ensure our theories are logically coherent, empirically evaluable, and broadly informative.

### Construct Falsifiable Theories

A good causal theory must be falsifiable—it should generate predictions that can be proven wrong by empirical evidence. Falsifiability requires specificity: rather than vague ideas, we should frame hypotheses in terms of observable implications. Even if a theory has been supported by many consistent tests, we must remain open to modifying or discarding it when contradictory evidence arises. This does not mean abandoning a theory at the first sign of inconsistency, but rather treating each empirical test as contributing information about the theory's validity and the boundaries of its applicability.

### Build Theories That Are Internally Consistent

Theories must not contain internal contradictions. If two parts of a theory logically generate incompatible hypotheses, then the theory is false by definition—even without any data.

However, formal models are only approximations of reality. While they clarify reasoning, they often involve simplifying assumptions—such as ignoring omitted variables—that are useful for abstraction but must be revisited when designing empirical tests. A model that is internally consistent but empirically irrelevant is of limited value. Thus, consistency must be paired with empirical awareness.

### Select Dependent Variables Carefully

Your choice of a dependent variable (outcome) is critical. First, make sure it is truly dependent—that is, it should not cause changes in the explanatory variables. Reverse causality undermines inference. Second, avoid selecting cases based on the value of the dependent variable (e.g., studying only instances of war when analyzing causes of war), as this produces selection bias.

Finally, ensure your outcome actually reflects the phenomenon you aim to explain. For example, controlling for gravity and then studying falling speed may help explore object-level differences, but tells us nothing about gravity itself. Likewise, in social science, we must ensure our outcomes capture meaningful variation relevant to the causal question at hand.

### Maximize Concreteness

Concrete and observable concepts should be used whenever possible. Abstract ideas like “motivation,” “national interest,” or “institutionalization” are often necessary, but they must be clearly defined and linked to measurable indicators. For instance, if a researcher wants to study “national interest,” they might define it in terms of measurable behaviors such as foreign aid allocation, military deployments, or trade agreements—concrete actions that reflect a government’s priorities.

The danger lies in reification — treating indirect or weak proxies as if they fully represent the abstract concept. For example, measuring “institutionalization” by the size of a staff may be convenient, but this should be justified and supported with other evidence. Overreliance on a single indicator risks tautology or circular reasoning.

### State Theories in as Encompassing Ways as Feasible

A good theory should apply beyond a single case — it should help explain a wide range of phenomena across time, space, and context. This increases a theory’s leverage, or its ability to account for variation in many settings.

However, broader theories must still be falsifiable and measurable. Stating that a pattern applies to "all legislatures" rather than just "the German Bundestag" is helpful only if the theory’s scope and mechanisms are clear. When applying a theory to new settings, we should ask: what conditions must hold for the theory to work here? Theories that travel well—and identify the limits of their applicability—are especially valuable.

## Wrapping Up: What We’ve Learned

In this module, we explored the foundations of causal inference, emphasizing the central idea that causal claims require a comparison between what actually occurred and what would have occurred under different conditions — the counterfactual. We examined how to define causality in terms of potential outcomes, the importance of identifying plausible causal mechanisms, and the challenges posed by multiple causation.

We also introduced the key assumptions necessary for valid causal inference, including unit homogeneity and conditional independence, and discussed how these assumptions help us navigate the fundamental problem of causal inference. Finally, we outlined best practices for constructing rigorous causal theories, focusing on clarity, concreteness, logical consistency, and testability.

With Modules 1 through 3 behind you, you're now equipped with the conceptual tools to begin applying these ideas in practice. Up next: a quiz and Problem Set 1, where you'll put your understanding. Be sure to complete these before moving on to the next module!

