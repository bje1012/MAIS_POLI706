---
title: "Inference and Prediction II: Parametric Bootstrapping"
output:
  html_document:
    theme: cerulean
    highlight: kate
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{=html}
<style>
body {
  font-size: 17px;
  line-height: 1.6;
}
</style>
```

```{r setup, include=FALSE}
rm(list = ls())   
library(ggeffects)
library(kernlab)
library(showtext)
library(kableExtra)
library(dplyr)
library(tidyverse)
library(scales)
library(tidyverse)
library(haven)
library(stargazer)
```


In the previous module, we explored non-parametric bootstrapping, a technique that resamples directly from the observed data without making assumptions about the underlying distribution. We used this method to estimate the uncertainty of regression coefficients by generating empirical sampling distributions through repeated resampling with replacement.

In this module, we focus on **parametric bootstrapping**, a simulation-based technique that relies on model-based assumptions about the data-generating process. 

The module is divided into two parts. In Part 1, we will learn the conceptual foundation of parametric bootstrapping, how it differs from the non-parametric version, and how we use simulations to understand uncertainty in parameter estimates and predicted values. In Part 2, we will implement parametric bootstrapping in R using the V-Dem dataset and interpret the resulting distributions of expected values.

By the end of this module, you will be able to:

- Understand the logic and purpose of parametric bootstrapping

- Distinguish between predicted values and expected values in simulation

- Generate and interpret bootstrapped expected values from a regression model in R

## Part 1: What is Parametric Bootstrapping?

**Parametric bootstrapping** is a simulation-based method used to assess the uncertainty of estimates or to approximate the sampling distribution of a statistic. It assumes that the data are generated from a known probability distribution (such as normal, Poisson, or negative binomial), but with unknown parameters. These parameters are first estimated from the observed data, and then multiple synthetic datasets are generated by drawing random samples from the fitted distribution.

### How is it different from non-parametric bootstrapping?

While both approaches aim to understand the variability of an estimate through repeated sampling, they differ in an important way:

| Feature | Parametric Bootstrapping | Non-Parametric Bootstrapping |
|--------|---------------------------|-------------------------------|
| Assumption | Assumes a specific distributional form (e.g., Normal, Poisson) | Makes no distributional assumption |
| Sampling | Samples from the estimated distribution | Resamples from the original dataset with replacement |

### What is Statistical Simulation?

Statistical simulation is a method used to approximate complex mathematical calculations, especially when dealing with probability distributions. It works similarly to how we use survey sampling in research. In survey research, we estimate characteristics of a population (like the mean or variance) by taking a random sample from it. The larger the sample size n, the more accurate our estimate becomes.

Simulation applies the same logic, but instead of learning about a population, we learn about a probability distribution. We do this by drawing random samples from the distribution and using them to estimate features like the mean, variance, or specific probabilities. The more simulations we run (larger M), the more accurate the approximation.

To estimate the mean of a distribution P(y), instead of solving a difficult integral, we can draw many values from P(y) and calculate their average. For example, let's say you're rolling a fair six-sided die. The probability distribution P(y) is simple: each outcome from 1 to 6 has an equal chance of 1/6.

Now suppose you want to know the expected value (mean) of a die roll. Mathematically, the expected value is 

$$
E(Y) = \sum_{y=1}^6 y\cdot P(y) = (1 + 2 + 3 +4 +5+6)/6 = 3.5
$$

But instead of computing this, you could simulate it: 

- Roll the die 10,000 times using a computer.

- Take the average of those rolls.

- The result will be very close to 3.5

You can use the same simulation to estimate the variance of the outcomes, the probability that a roll is greater than 4, and even a confidence interval for a function of the roll, like the square of the number rolled.

You can make your estimates as precise as you want by increasing the number of simulations. To check precision, repeat the same simulation multiple times. If the results stay consistent, your estimate is stable.

In sum, statistical simulation allows you to estimate features of a probability distribution—such as the mean, variance, probabilities, and confidence intervals—without complex math. Instead, you rely on repeated random draws from the distribution. A simple example is simulating die rolls. The more simulations you run, the more accurate your results.

### Simulating the Parameters

When we estimate a statistical model, the coefficients we obtain - such as $\hat{\beta}$ and $\hat{\alpha}$ - are not exact values. They are subject to uncertainty because they are based on a finite sample. To reflect this uncertainty, we simulate many plausible versions of these parameters by drawing them from their sampling distribution.

These simulated parameters represent values that are all consistent with the data and the model, but they vary around the original estimates, capturing our uncertainty about what the true values might be. To do this, we need two elements: the point estimates of the parameters and their variance-covariance matrix. We collect all parameters into a single vector, $\hat{\gamma}$, and use the Central Limit Theorem to justify simulating values of $\gamma$ from a multivariate normal distribution with mean $\hat{\gamma}$ and variance $\hat V(\gamma)$. In notation, this is written as:

$$
\tilde{\gamma} \sim N(\hat{\gamma}, \hat V(\hat\gamma)).
$$

Practically, we simulate one set of parameters by first estimating the model and recording the point estimates and their variance-covariance matrix. Then, we draw a value of $\gamma$ from the multivariate normal distribution defined above. By repeating this process many times - for example, $M = 1000$ times - we generate 1000 plausible sets of parameters.

If we had perfect knowledge of the true parameter values, all simulated sets would be identical. But since we do not, the variability among the simulated sets reflects how much uncertainty remains. This pattern of variation captures all the information available from our model and data about the parameters.

### Predicted Values vs. Expected Values

#### Predicted Values

Our task is to draw one possible value of the outcome variable $Y$ conditional on a chosen set of explanatory variables, denoted $X_c$. Let the simulated parameters be denoted by $\tilde{\theta}$, and the resulting predicted value by $\tilde{Y}_c$.

Predicted values can take several forms depending on the context of $X_c$. For example:

-  If $X_c$ represents a future situation, $\tilde{Y}_c$ is a simulated forecast.

- If $X_c$ is based on observed data, $\tilde{Y}_c$ is a simulated predicted value.

- If $X_c$ reflects a hypothetical scenario, $\tilde{Y}_c$ becomes a simulated counterfactual prediction.

Importantly, $\tilde{Y}_c$ is not the same as the expected value $\hat{Y}$ in a linear regression, which represents the mean response. Instead, $\tilde{Y}_c$ includes randomness from both the systematic and stochastic components of the model.

To simulate one predicted value, follow this procedure:

1.  Using the algorithm from the previous subsection, draw one value of the parameter vector $\tilde{\gamma} = \text{vec}(\tilde\beta, \tilde\alpha)$ from the multivariate normal distribution.
  
2. Decide the context of prediction (forecast, observed, or hypothetical), and choose one set of explanatory variable values. Denote this vector as $X_c$.
  
3. Using the simulated effect coefficients, compute the systematic component: $\tilde\theta_c = g(X_c, \tilde\beta)$, where $g(\cdot)$ is the model’s systematic function.
  
4.  Draw one random value $\tilde{Y}_c$ from the distribution $f(\tilde \theta_c, \tilde\alpha)$, representing the stochastic component of the model.

Repeat this process $M = 1000$ times to obtain 1000 simulated predicted values. These simulations allow us to approximate the full probability distribution of $Y_c$. From this distribution, we can compute not only the average predicted value but also measures of uncertainty, such as confidence intervals or prediction intervals.

#### Expected Values 

Depending on the research question, the expected (or mean) value of the dependent variable may be more informative than a single predicted value. A predicted value incorporates both fundamental and estimation uncertainty, while an expected value averages out the fundamental variability due to randomness, leaving only the uncertainty from finite sample estimation.

For example, imagine you are trying to estimate the weight of an apple from a specific orchard, where all apples are grown under similar conditions.


-  The expected value is: "On average, an apple from this orchard weighs 200g." 

It reflects the mean across many apples under the same conditions.
  
-  The predicted value is: "This particular apple I just picked is likely to weigh 212g."  
  
It reflects a single realization, including randomness.

In statistical models:

- The expected value summarizes the average or systematic effect of explanatory variables (e.g., "Democracies are less likely to go to war, on average").

- The predicted value estimates what might happen in a specific case, incorporating both systematic effects and random variation (e.g., "This particular democracy has a 12\% chance of entering a conflict next year").

To simulate one expected value, follow this procedure:

1. Draw one value of the parameter vector $\tilde{\gamma} = \text{vec}(\tilde\beta, \tilde\alpha)$.
  
2. Choose a set of explanatory variable values and denote the vector as $X_c$.
  
3. Using the simulated effect coefficients from $\beta$, compute the systematic component: $\theta_c = g(X_c, \tilde\beta)$, where $g(\cdot)$ represents the model's systematic function.

4. Simulate $m$ values of the outcome variable $\tilde Y_c^{(k)}$ for $k = 1, \dots, m$ from the stochastic component $f(\theta_c, \tilde\alpha)$. This step simulates fundamental uncertainty.
  
5. Compute the mean of these $m$ simulated values to obtain a single expected value:

$$
  \tilde{E}(Y_c) = \frac{1}{m} \sum_{k=1}^{m} Y_c^{(k)}.
$$

When $m = 1$, this algorithm reduces to the predicted value simulation. As $m$ increases, the algorithm better captures and averages out the fundamental variability, producing a more precise expected value. To generate $M = 1000$ simulations of the expected value, repeat the entire procedure above $M$ times for a fixed $m$. The variation across these $M$ expected values reflects estimation uncertainty, and the resulting distribution approximates the full probability distribution of $\tilde E(Y_c)$. This allows the researcher to calculate summary statistics such as means, standard errors, and confidence intervals.

While this approach works for all statistical models, it may involve approximation error, which can be reduced by choosing sufficiently large values of $m$ and $M$. For certain models, including the linear-normal and logit models, where $E(Y_c) = \theta_c$, steps 4 and 5 may be skipped altogether. In such cases, the first three steps alone suffice to simulate the expected value, offering a shortcut that saves both time and computation.

## Part 2: Parametric Bootstrapping in R

Now that we understand the theory behind parametric bootstrapping, let's implement it in R using a linear regression model. The goal here is to simulate the uncertainty in our coefficient estimates and predicted values based on the parametric assumptions of the model.

We’ll use the vdem dataset to estimate a linear regression model predicting the Electoral Democracy Index (`v2x_polyarchy`) using logged GDP per capita and civil war status.

**Step 1: Load data and estimate the model**

```{r, message=FALSE}
vdem <- read.csv("vdem.csv") %>% 
  drop_na() %>%
  mutate(loggedgdp = log(e_gdppc + 0.0001))

model1 <- lm(v2x_polyarchy ~ loggedgdp + e_civil_war, data = vdem)
stargazer(model1, type= "text")
```

**Step 2: Extract estimated coefficients and variance-covariance matrix**

```{r}
coefs <- coef(model1)  # point estimates
vcov_matrix <- vcov(model1)  # variance-covariance matrix
coefs
vcov_matrix
```

**Step 3: Simulate parameter sets from multivariate normal**

We simulate 1000 parameter vectors from the multivariate normal distribution defined by our coefficient estimates and their variance-covariance matrix.

```{r, message=FALSE}
library(MASS)
set.seed(123)
M <- 1000
sim_params <- MASS::mvrnorm(n = M, mu = coefs, Sigma = vcov_matrix)
```

**Step 4: Compute expected values for a hypothetical country**

Suppose we want to compute the expected EDI for a country with:
- logged GDP = 8
- not in civil war (e_civil_war = 0)

```{r}
X <- c(1, 8, 0)  # intercept, loggedgdp, e_civil_war
exp_vals <- sim_params %*% X  # matrix multiplication
```

Now we can summarize the results:

```{r}
mean(exp_vals)  # mean expected EDI
sd(exp_vals)    # standard error (standard deviation of simulated expected values)
quantile(exp_vals, c(0.025, 0.975))  # 95% CI
```

The mean value represents the average expected EDI across 1,000 simulations, providing our best estimate for a country with a logged GDP of 8 and no civil war. The standard error captures the uncertainty around this estimate, reflecting how much it might vary due to the sampling variability in the model’s parameters. In other words, even with the same country characteristics, the predicted EDI could shift depending on the sample used to fit the model. Using this uncertainty, we construct a 95% confidence interval, which suggests that the expected EDI for such a country is likely to fall between 1.605 and 1.662.

**Step 5: Visualize the distribution of expected values**

```{r, message=FALSE}
data.frame(EDI = exp_vals) %>%
  ggplot(aes(x = EDI)) +
  geom_histogram(fill = "gray", color = "black") +
  geom_vline(xintercept = mean(exp_vals), color = "red", linetype = "dashed") +
  labs(title = "Distribution of Simulated Expected EDI Values",
       x = "\nExpected EDI", y = "Frequency\n") +
  theme_bw()
```

This plot visually confirms that the majority of the simulated expected EDI values are concentrated around the mean of approximately 1.635. While there is some variability due to uncertainty in the model’s parameter estimates, the spread is relatively narrow, with most values falling between roughly 1.60 and 1.67. The distribution is symmetric and bell-shaped, which aligns well with the normality assumption underlying parametric bootstrapping.

## Wrapping Up: What We’ve Learned

In this module, we explored the concept and implementation of parametric bootstrapping.

We learned how to simulate parameter values from the estimated multivariate normal distribution of a model’s coefficients, and how to use these simulations to compute expected outcomes for hypothetical scenarios. This approach allows us to quantify uncertainty in predictions and better understand the variability in our estimates under the assumptions of the model.

In the next module, we will turn to model diagnostics—an essential step in evaluating whether your model’s assumptions hold and whether its estimates can be trusted. Before you move on, be sure to complete the quiz and problem set to solidify your understanding of parametric bootstrapping and simulation!
