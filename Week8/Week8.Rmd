---
title: "Ordinary Least Squares III: Application"
output:
  html_document:
    theme: cerulean
    highlight: kate
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{=html}
<style>
body {
  font-size: 17px;
  line-height: 1.6;
}
</style>
```

```{r setup, include=FALSE}
library(kernlab)
library(showtext)
library(kableExtra)
library(dplyr)
library(tidyverse)
library(scales)
library(tidyverse)
```


In the last two modules, we explored the fundamentals of simple (bivariate) linear regression. We learned how to estimate relationships between two variables, interpret regression coefficients, construct confidence intervals, conduct hypothesis tests, and understand key model assumptions.

In this module, we will expand our understanding by learning about multiple regression, which allows us to include two or more explanatory variables in the same model. We’ll also learn how to carry out regression analysis using R.

In Part 1, we’ll begin by reviewing bivariate regression and then introduce the logic and structure of multiple regression. In Part 2, we’ll discuss important considerations when building multiple regression models—such as ensuring that independent variables differ, understanding and identifying collinearity, and assessing model fit using $R^2$ and adjusted $R^2$. In Part 3, we’ll walk through how to run a multiple regression model in R and interpret the output correctly.

By the end of this module, you’ll be able to:

- Understand the multiple regression

- Identify potential problems like collinearity

- Evaluate model fit and use R to estimate and interpret multiple regression models

## Part 1: Bivariate Regression and Multiple Regression

In this part, we focus on building from your knowledge of simple linear regression to understand how multiple regression works. We begin by reviewing the key concepts behind bivariate regression, which models the relationship between one dependent variable and one independent variable. This type of analysis helps us answer questions like: Does EMB autonomy influence electoral democracy? or How does economic development affect democracy level? While bivariate regression is useful for identifying associations between two variables, it does not account for the influence of other factors that might also affect the outcome. This is where multiple regression comes in. Multiple regression expands on the bivariate framework by allowing us to include two or more explanatory variables in the model. This is crucial in social science research, where outcomes are rarely driven by a single factor. 

### What Is Multiple Regression, and How Is It Different from Bivariate Regression?

In the last two modules, we learned about Ordinary Least Squares (OLS) regression using a bivariate model, where we included only one explanatory variable. The slope coefficient in a simple regression gives the marginal relationship between the dependent variable and a single independent variable.

The statistical model for a **simple (bivariate) linear regression** is:

$$
Y_i = A + B X_i + E_i
$$


- $Y_i$ is the **dependent variable** ,
- $X_i$ is the **independent variable**,
- $A$ is the **intercept**, which represents the predicted value of $Y$ when $X = 0$,
- $B$ is the **slope coefficient**, showing the average change in $Y$ for a one-unit increase in $X$,
- $E_i$ is the **error term**.

The OLS method estimates $A$ and $B$ by minimizing the sum of squared errors:

$$
S(A, B) = \sum_{i=1}^nE^2 = \sum(Y_i - A - B X_i)^2
$$

#### Extending to Multiple Regression

Most of the intuition and interpretation we learned in bivariate regression carries over to multiple regression, which allows us to estimate the effect of two or more explanatory variables on the dependent variable.

The general statistical model for multiple regression is:

$$
Y_i = A + B_1 X_{i1} + B_2 X_{i2} + \cdot \cdot \cdot + B_k X_{ik} + E_i
$$

- $X_{i1}, X_{i2}, \dots, X_{ik}$ are k different independent variables for observation $i$
- $B_1, B_2, \dots, B_k$ are the corresponding slope coefficients
- Each coefficient $B_k$ represents the partial effect of $X_k$ on $Y$, holding all other variables constant

Suppose we want to explain a dependent variable ($Y$) using two explanatory variables ($X_1$ and $X_2$). The model would be:

$$
Y_i = A + B_1 X_{i1} + B_2 X_{i2} + E_i
$$

The least-squares criterion for this model becomes:

$$
S(A, B_1, B_2) = \sum_{i=1}^nE^2 = \sum(Y_i - A - B_1 X_{i1} - B_2 X_{i2})^2
$$

OLS will choose values of $A$, $B_1$, and $B_2$ that minimize this quantity, giving us the best-fitting plane through the data.

## Part 2: Considerations When Adding Multiple Variables to a Model

Now that we understand what multiple regression is and how it extends the simple regression framework, it’s important to consider what happens when we include more than one explanatory variable in our model. However, wee need to consider that adding more variables introduces potential issues that can affect the interpretation and performance of the model.

### $X_1$ and $X_2$ must differ

Although the process of estimating a multiple regression model with two or more predictors is similar to that of a simple regression, there is one major conceptual difference:

The slope coefficients for the explanatory variables in multiple regression are partial coefficients, while the slope coefficient in simple regression gives the marginal relationship between the response variable and a single explanatory variable.

That is, each slope in multiple regression represents the ‘effect’ on the response variable of a one-unit increment in the corresponding explanatory variable holding constant the value of the other explanatory variable. The simple-regression slope effectively ignores the other explanatory variable.

For example, in the model:

$$
Y_i = A + B_1 X_{i1} + B_2 X_{i2} + E_i
$$

- $B_1$ shows the effect of a one-unit increase in $X_1$ while keeping $X_2$ constant

- $B_2$ shows the effect of a one-unit increase in $X_2$ while keeping $X_1$ constant

This means that the explanatory variables must differ. If $X_1$ and $X_2$ are highly similar or perfectly correlated, the model won’t be able to distinguish their effects, which leads us to the next issue.

### Collinearity

Collinearity (or multicollinearity) occurs when two or more explanatory variables in the model are highly correlated. In other words, the model can’t tell which variable is driving the effect on $Y$ because the variables are too similar.

This is a problem because:

- It inflates the standard errors of the coefficients,

- Makes coefficient estimates unstable and unreliable,

- Makes it harder to assess which predictors are truly significant.

To diagnose multicollinearity, we can use the VIF score:

$$
\text{VIP} = \frac{1}{1-R_j^2}
$$

- $R_j^2$ is the R-squared value from regressing variable $X_j$ on all the other explanatory variables

- A VIF of 1 means no correlation with other variables

- A VIF of 5 or more is a red flag

For instance, suppose we want to include both EMB autonomy (v2elembaut) and EMB capacity (v2elembcap) as explanatory variables in the model. Before adding them, we should check whether these variables exhibit multicollinearity.

We can use the VIF score to diagnose this issue:

```{r, message=FALSE}
vdem <- read.csv("vdem.csv") %>% drop_na()

# Filter V-Dem data to year 2000
vdem_2000 <- vdem %>%
  filter(year == 2000)

fit <- lm(v2x_polyarchy ~ v2elembaut + v2elembcap, data = vdem_2000)

# Load the car package and compute VIF scores
library(car)
vif(fit)
```

The VIF scores for both variables are approximately 1.83. This value is well below the common thresholds of 5, we can conclude that there is no significant multicollinearity between EMB autonomy and EMB capacity in this model, which means it is safe to include both variables in the regression.

### Model fit 

Just like in simple regression, we can assess how well our multiple regression model explains the variation in the dependent variable by using $R^2$. 

The formula is:

- Total Sum of Squares (TSS): $TSS = \sum(Y_i - \bar Y)^2$

- Residual Sum of Squares (RSS): $RSS = \sum(Y_i - \hat Y_i)^2$

- Regression Sum of Squares (RegSS): $RegSS = TSS - RSS$

Then:

$$
R^2 = \frac{RegSS}{TSS}
$$

However, one important thing to remember is that $R^2$ always increases when more predictors are added — even if those predictors do not actually improve the model. That’s why we use Adjusted $R^2$, which adjusts for the number of predictors:

- If you add a variable that truly improves the model, adjusted $R^2$ will go up.

- If the new variable does not help, adjusted $R^2$ can actually decrease.

So when evaluating model fit in multiple regression, always check adjusted $R^2$, not just $R^2$.

## Part 3: Running Multiple Regression Models in R

So far, we’ve learned the logic behind multiple regression, how to interpret coefficients, and what to watch out for when adding multiple predictors into a model. Now, we’ll put that knowledge into practice by fitting both bivariate and multiple regression models in R.

### Bivariate Model

Let’s begin by examining the relationship between Electoral Management Body (EMB) Autonomy (`v2elembaut`) and the Electoral Democracy Index (`v2x_polyarchy`). We’ll start by visualizing the relationship using a scatterplot.

```{r}
# Load needed package 
library(tidyverse)

ggplot(vdem_2000, aes(x = v2elembaut, y = v2x_polyarchy)) +
  geom_point(alpha = 0.8, color = "black") +
  labs(title = "Electoral Management Autonomy and Electoral Democracy Index (2000)",
       x = "\nEMB Autonomy",
       y = "Electoral Democracy Index\n") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

This plot shows how scores of EMB autonomy relate to scores on the Electoral Democracy Index (EDI) for countries in the year 2000. If the dots show a clear upward trend that gives us an idea of whether a linear relationship might exist.

Now, let’s add a regression line to the scatterplot using `geom_smooth(method = "lm")`. This shows the best-fitting line through the data, estimated using Ordinary Least Squares (OLS).

```{r}
ggplot(vdem_2000, aes(x = v2elembaut, y = v2x_polyarchy)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Electoral Management Autonomy and Electoral Democracy Index (2000)",
       x = "\nEMB Autonomy",
       y = "Electoral Democracy Index\n") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

This plot helps us visually confirm the presence of a linear relationship between the two variables. As the blue line in the plot shows, there is a positive linear relationship between EMB Autonomy and the EDI, which means that countries with higher EMB autonomy tend to score higher on the EDI.

Now we estimate the regression model using R’s built-in lm() function.

```{r}
# Fit the linear model
model <- lm(v2x_polyarchy ~ v2elembaut, data = vdem_2000)
```

This line of code tells R to model v2x_polyarchy as a function of v2elembaut using linear regression.

To inspect the results, we can use the summary() function:

```{r}
summary(model)
```

For cleaner and more readable regression output, we can use the stargazer package.

```{r, warning=FALSE, message=FALSE}
# install.packages("stargazer")
library(stargazer)
```

Now let’s display the regression results:

```{r, warning=FALSE}
stargazer(model, type = "text")
```

The estimated slope coefficient for EMB autonomy is 0.139, indicating that a one-point increase in EMB autonomy is associated with a 0.139-point increase in the EDI. This estimate is statistically significant, as denoted by the triple stars (***) beside the coefficient, which indicate a p-value less than 0.01. The standard error of the estimate is 0.005, suggesting that the coefficient is estimated with a high degree of precision.

The intercept is 0.439, which represents the predicted value of EDI when EMB autonomy is zero. This figure is also statistically significant at the 1% level.

The model's R-squared value is 0.849, meaning that approximately 85% of the variation in electoral democracy across countries can be explained by differences in EMB autonomy. The adjusted R-squared, which accounts for the number of predictors in the model, is virtually identical at 0.848.

The residual standard error (RSE) is 0.097, indicating that the typical prediction error made by the model is relatively small—less than 0.1 points on the EDI scale. 

Now that we understand the strength and significance of the relationship, we can visualize the predicted effect of EMB autonomy on the Electoral Democracy Index. To do this, we use the `ggeffects` package in R, which allows us to generate and plot marginal effects based on our regression model. The plot below shows the predicted EDI scores for different levels of EMB autonomy, along with a 95% confidence interval around the estimates:

```{r, warning=FALSE, message=FALSE}
# install.packages("ggeffects")
library(ggeffects)
```

```{r}
eff1 <- ggpredict(model, terms = "v2elembaut")

plot(eff1, colors = "blue")+
  labs(title = NULL,
       x = "\nEMB Autonomy",
       y = "Electoral Democracy Index\n") +
  theme_minimal()
```

This visualization reinforces our earlier findings: as EMB autonomy increases, so does the predicted level of EDI. The slope of the line reflects the estimated coefficient (0.139), and the narrow confidence band signals that these predictions are made with a high degree of certainty.

### Multiple Regression Model

So far, we’ve looked at a bivariate model that includes only one explanatory variable. Now, we want to expand the model by adding another explanatory variable: GDP per capita (`e_gdppc`). This will help us examine whether the relationship between EMB autonomy and electoral democracy holds even after controlling for a country's economic development.

However, before including GDP in our model, we must first consider its distribution. GDP per capita is typically highly skewed: a few countries have very high incomes, while many others have low or moderate values. This kind of skewed distribution can distort regression results, so it's common practice to transform GDP per capita by taking its natural logarithm.

Let’s first examine the distribution of the original `e_gdppc` variable:

```{r, message=FALSE}
ggplot(vdem_2000, aes(x = e_gdppc)) +
  geom_histogram(fill = "#275D8E") +
  labs(title = "", x = "\nGDP per capita",  y = "Frequency") +
  theme_minimal()
```

As shown in the histogram, the distribution of GDP per capita is right-skewed: most countries have relatively low values, while a small number have very high values. To reduce this skew and compress the wide range of values, we take the logarithm of GDP per capita. This transformation brings high values closer together and makes the distribution more symmetrical.

We also add a small constant (0.0001) before applying the log function to avoid issues with taking the log of zero:

```{r}
vdem_2000 <- vdem_2000 %>%
  mutate(loggedgdp = log(e_gdppc + 0.0001))
```

Let’s now compare the distributions of the original and logged GDP values:

```{r, message=FALSE, fig.align='center'}
p1 <- ggplot(data = vdem_2000, aes(x = e_gdppc)) +
  geom_histogram(fill = "#275D8E") +
  labs(title = "", x = "GDP per Capita", y = "Frequency") +
  theme_minimal()

p2 <- ggplot(data = vdem_2000, aes(x = loggedgdp)) +
  geom_histogram(fill = "#275D8E") +
  labs(title = "", x = "logged GDP per Capita", y = "Frequency") +
  theme_minimal()

library(gridExtra)
# Arrange the two plots side by side
grid.arrange(p1, p2, ncol = 2)
```

As we can see from the side-by-side histograms, the logged GDP per capita variable has a much more symmetric distribution. This transformation helps meet the assumptions of linear regression—particularly the assumption that residuals are normally distributed and that relationships between variables are linear.

Now, let’s add the logged GDP variable to our regression model:

```{r}
multimodel1 <- lm(v2x_polyarchy ~ v2elembaut + loggedgdp, data = vdem_2000)
stargazer(multimodel1, type = "text")
```

The table above presents the results of a multiple linear regression analysis predicting the EDI using two explanatory variables: EMB autonomy and logged GDP per capita. Both predictors are statistically significant at the 0.01 level, suggesting that each variable is independently associated with the level of EDI, even when controlling for the other.

The coefficient for EMB autonomy is 0.125. This means that, holding GDP constant, a one-unit increase in EMB autonomy is associated with a 0.125-point increase in the EDI. This relationship is both statistically significant, confirming our earlier findings in the bivariate model that greater autonomy in electoral management bodies is strongly linked to higher levels of electoral democracy.

Next, the coefficient for logged GDP per capita is 0.042. This indicates that, holding EMB autonomy constant, a one-unit increase in the log of GDP per capita is associated with a 0.042-point increase in the EDI. This means that more economically developed countries tend to score higher on measures of electoral democracy.

The intercept of 0.372 represents the predicted value of the Electoral Democracy Index when both EMB autonomy and logged GDP are zero.

In terms of model fit, the R-squared value is 0.875, meaning that the two predictors together explain 87.5% of the variation in the EDI across countries in this sample. The adjusted R-squared, which adjusts for the number of predictors, is nearly identical at 0.873, indicating that both variables contribute meaningfully to the model.

To better understand the effect of economic development on electoral democracy, we visualize the predicted values of the EDI across the range of logged GDP per capita. The plot displays the expected EDI scores for different levels of economic development, holding EMB autonomy constant.

```{r}
eff2 <- ggpredict(multimodel1, terms = "loggedgdp")

plot(eff2, colors = "blue")+
  labs(title = NULL,
       x = "\nLogged GDP per capita",
       y = "Electoral Democracy Index\n") +
  theme_minimal()
```

The positive slope of the line confirms our earlier regression findings: as logged GDP per capita increases, countries are predicted to have higher scores on the EDI. The confidence band around the line shows the 95% uncertainty range, indicating that this upward trend is statistically reliable across the range of GDP values.

To further explore factors that may influence electoral democracy, we add a third explanatory variable to our model: `e_civil_war`, a binary (dummy) variable that indicates whether a civil war was ongoing in the country during the year 2000.

The updated regression model is:

```{r}
multimodel2 <- lm(v2x_polyarchy ~ v2elembaut + loggedgdp + factor(e_civil_war), data = vdem_2000)
stargazer(multimodel1, multimodel2, type = "text")
```

This second model allows us to estimate the effect of civil conflict while still accounting for EMB autonomy and economic development. By using `factor(e_civil_war)`, we ensure that the dummy variable is treated as a categorical predictor, enabling the model to compare countries experiencing civil war to those not in conflict.

Across both models, EMB autonomy maintains a strong, positive, and statistically significant effect on electoral democracy. In Model 2, the coefficient of 0.126 suggests that a one-unit increase in EMB autonomy is associated with a 0.126 point increase in the EDI, holding other variables constant.

Logged GDP per capita also shows a positive and statistically significant relationship with electoral democracy in both models. The coefficient of around 0.41 indicates that for each one-unit increase in the natural log of GDP per capita, the EDI score increases by about 0.041 points, controlling for EMB autonomy and civil war.

In Model 2, the civil war variable is negative and statistically significant at the 5% level (–0.081). This means that, all else equal, countries experiencing civil war had EDI scores about 0.08 points lower than countries not experiencing civil conflict.

The overall model fit improves slightly with the inclusion of the civil war variable. The $R^2$ increases from 0.875 in Model 1 to 0.880 in Model 2, and the adjusted $R^2$ also increases, indicating that the added variable improves explanatory power without overfitting. The residual standard error decreases from 0.089 to 0.087, suggesting that the model’s predictions become slightly more accurate with the inclusion of civil war.

Since `e_civil war` is a dummy variable, the way we interpret and visualize its effect is different from that of a continuous variable.

```{r}
eff3 <- ggpredict(multimodel2, terms = "e_civil_war")

plot(eff3, colors = "blue")+
  scale_x_continuous(breaks = c(0, 1)) +
  labs(title = NULL,
       x = "\nLogged GDP per capita",
       y = "Electoral Democracy Index\n") +
  theme_minimal()
```

The plot shows the predicted values of the EDI for two groups:

- Countries not experiencing civil war (where `e_civil_war` = 0)

- Countries experiencing civil war (where `e_civil_war` = 1)

The plot shows that the predicted EDI is higher for countries not experiencing civil war, approximately 0.55, with a narrow 95% confidence interval, indicating a precise estimate. In contrast, for countries experiencing civil war, the predicted EDI score is lower, around 0.47. The gap between these two predicted values—approximately 0.08—aligns with the estimated regression coefficient for e_civil_war, confirming that civil war is associated with a notable reduction in electoral democracy.

Additionally, the confidence interval for countries experiencing civil war is wider, suggesting greater uncertainty in the predicted EDI score for this group. This is due to fewer observations in the political conditions of war-affected countries in our dataset.

Importantly, the confidence intervals for the two groups do not overlap, which provides strong evidence that the difference in EDI scores between countries with and without civil war is statistically significant. In other words, the negative association between civil war and electoral democracy is unlikely to be due to random chance.

## Wrapping Up: What We’ve Learned

In this module, we extended our understanding of linear regression by moving from simple (bivariate) models to multiple regression. We began by reviewing the foundations of bivariate regression, including how to estimate and interpret relationships between a dependent and a single independent variable.

We then explored the logic and structure of multiple regression, learning how to include multiple explanatory variables in a model to isolate the effect of each predictor while holding others constant. This allowed us to assess more realistic, multifactor explanations of political outcomes like electoral democracy.

In the applied portion of the module, we practiced running multiple regression models using real-world data from the V-Dem dataset. We saw how to visualize relationships between variables using scatterplots and regression lines, estimate linear models in R using the lm() function, and interpret output using summary(), stargazer, and ggeffects. 

By mastering these concepts, you are now equipped not only to run multiple regression models in R, but also to evaluate their quality and interpret their results. You’ve learned how to handle model diagnostics, transform variables when appropriate, and draw valid conclusions from regression output.

Next, you will have the opportunity to reinforce these concepts through a short quiz and Problem Set 6. Make sure to complete them before moving on to the next module, where we will begin exploring Generalized Linear Modeling (GLM) and Maximum Likelihood.
